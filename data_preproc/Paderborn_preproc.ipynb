{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9051fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import torch\n",
    "\n",
    "def set_seed_and_deterministic(seed):\n",
    "    \"\"\"\n",
    "    Sets the seed for NumPy and PyTorch and makes PyTorch operations deterministic.\n",
    "\n",
    "    Parameters:\n",
    "    seed (int): The seed value to be set for reproducibility.\n",
    "    \"\"\"\n",
    "    # Set seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Set seed for PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # If using CUDA\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
    "\n",
    "    # Ensure deterministic behavior in PyTorch\n",
    "    # Note: This might impact performance and is not guaranteed for all operations\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Usage\n",
    "set_seed_and_deterministic(42)  # Replace 42 with your desired seed value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311c6e2d",
   "metadata": {},
   "source": [
    "# DataFolder Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55a2fcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEALTHY BEARINGS\n",
      "ARTIFICIAL DAMAGES\n",
      "REAL DAMAGES\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating a dictionary to represent healthy bearings data.\n",
    "# 'Folder_Name' lists the names of the folders containing data about healthy bearings.\n",
    "# 'Damage' labels each folder as 'Healthy'.\n",
    "healthy_dict = {\n",
    "    \"Folder_Name\": [\"K001\", \"K002\", \"K003\", \"K004\", \"K005\", \"K006\"],\n",
    "    \"Damage\": [\"Healthy\", \"Healthy\", \"Healthy\", \"Healthy\", \"Healthy\", \"Healthy\"]\n",
    "}\n",
    "\n",
    "# Converting the dictionary to a Pandas DataFrame for better data representation.\n",
    "healthy_df = pd.DataFrame(healthy_dict)\n",
    "\n",
    "# Printing the DataFrame for healthy bearings.\n",
    "print(\"HEALTHY BEARINGS\")\n",
    "# print(healthy_df)\n",
    "\n",
    "# Creating a dictionary for bearings with artificial damages.\n",
    "# 'Folder_Name' lists the names of the folders containing artificially damaged bearings data.\n",
    "# 'Damage' labels each folder based on the type of damage (Outer or Inner).\n",
    "artificial_dict = {\n",
    "    \"Folder_Name\": [\"KA01\", \"KA03\", \"KA05\", \"KA06\", \"KA07\", \"KA09\", \"KI01\", \"KI03\", \"KI05\", \"KI07\", \"KI08\"],\n",
    "    \"Damage\": [\"Outer\", \"Outer\", \"Outer\", \"Outer\", \"Outer\", \"Outer\", \"Inner\", \"Inner\", \"Inner\", \"Inner\", \"Inner\"]\n",
    "}\n",
    "\n",
    "# Converting the dictionary to a Pandas DataFrame.\n",
    "artificial_df = pd.DataFrame(artificial_dict)\n",
    "\n",
    "# Printing the DataFrame for bearings with artificial damages.\n",
    "print(\"ARTIFICIAL DAMAGES\")\n",
    "# print(artificial_df)\n",
    "\n",
    "# Creating a dictionary for bearings with real damages.\n",
    "# 'Folder_Name' lists the names of folders containing data about bearings with real damages.\n",
    "# 'Damage' labels each folder based on the type of damage (Outer or Inner).\n",
    "real_dict = {\n",
    "    \"Folder_Name\": [\"KA04\", \"KA15\", \"KA16\", \"KA22\", \"KA30\", \"KI04\", \"KI14\", \"KI16\", \"KI17\", \"KI18\", \"KI21\"],\n",
    "    \"Damage\": [\"Outer\", \"Outer\", \"Outer\", \"Outer\", \"Outer\", \"Inner\", \"Inner\", \"Inner\", \"Inner\", \"Inner\", \"Inner\"]\n",
    "}\n",
    "\n",
    "# Converting the dictionary to a Pandas DataFrame.\n",
    "real_df = pd.DataFrame(real_dict)\n",
    "\n",
    "# Printing the DataFrame for bearings with real damages.\n",
    "print(\"REAL DAMAGES\")\n",
    "# print(real_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c719471",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f565c22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), \"./Paderborn Dataset\")\n",
    "N_POINTS = 249600 #? 64k Sampling rate for 3.9 seconds\n",
    "artificial_dataset = [\"K001\",\n",
    "                      \"KA01\", \"KA03\", \"KA05\", \"KA07\", \"KI01\", \"KI03\", \"KI07\"]\n",
    "real_dataset = [\"K001\",\n",
    "               \"KA04\", \"KB23\", \"KB27\", \"KI04\"]\n",
    "SAMPLE_LEN = 1024\n",
    "STRIDE = SAMPLE_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f746af70-649d-43bb-85fc-0de4541f80c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_list):\n",
    "    \"\"\"\n",
    "    Extracts and labels data from the given dataset list.\n",
    "    Each folder is expected to contain 4 conditions with 20 measurements each.\n",
    "    \"\"\"\n",
    "    # Dictionary to hold the data for each condition\n",
    "    conditions = {cond: {\"x\": np.zeros((len(dataset_list) * 20, N_POINTS)), \"y\": np.zeros(len(dataset_list) * 20)} for cond in [\"P1\", \"P2\", \"P3\", \"P4\"]}\n",
    "    condition_files = {\"N15_M07_F10\": \"P1\", \"N09_M07_F10\": \"P2\", \"N15_M01_F10\": \"P3\", \"N15_M07_F04\": \"P4\"}\n",
    "\n",
    "    for label, foldername in enumerate(dataset_list):\n",
    "        folder_path = os.path.join(data_dir, foldername)\n",
    "        file_path_list = glob(os.path.join(folder_path, \"*.mat\"))\n",
    "\n",
    "        # Assuming each folder contains an equal number of files for each condition\n",
    "        files_per_condition = len(file_path_list) // len(condition_files)\n",
    "        for file_index, file_path in enumerate(file_path_list):\n",
    "            for file_cond, cond in condition_files.items():\n",
    "                if file_cond in file_path:\n",
    "                    vibration_data = load_mat(file_path)\n",
    "                    sample_id = (label * 20) + (file_index % files_per_condition)\n",
    "                    conditions[cond][\"x\"][sample_id] = vibration_data\n",
    "                    conditions[cond][\"y\"][sample_id] = label\n",
    "                    break\n",
    "\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    for cond in conditions.values():\n",
    "        cond[\"x\"], cond[\"y\"] = torch.from_numpy(cond[\"x\"]), torch.from_numpy(cond[\"y\"])\n",
    "\n",
    "    return conditions\n",
    "\n",
    "def load_mat(mat_file):\n",
    "    \"\"\"\n",
    "    Loads vibration data from a .mat file.\n",
    "    \"\"\"\n",
    "    file_name = os.path.splitext(os.path.basename(mat_file))[0]\n",
    "    mat_file_array = loadmat(mat_file)\n",
    "    vibration_data = mat_file_array[file_name][\"Y\"][0][0][0][6][2][:, :N_POINTS]\n",
    "    return np.array(vibration_data)\n",
    "\n",
    "def sample(x, y):\n",
    "    output_x = x.unfold(1, SAMPLE_LEN, STRIDE)\n",
    "    output_x = output_x.contiguous().view(-1, SAMPLE_LEN)\n",
    "    windows_per_sample = output_x.size(0) // y.size(0)\n",
    "    output_y = y.repeat_interleave(windows_per_sample)\n",
    "    return output_x, output_y\n",
    "\n",
    "def sample_data(input_data):\n",
    "    # Applying the sample function to each condition in the dictionary\n",
    "    for condition, datasets in input_data.items():\n",
    "        sampled_x, sampled_y = sample(datasets[\"x\"], datasets[\"y\"])\n",
    "        input_data[condition] = {\"x\": sampled_x, \"y\": sampled_y}\n",
    "    \n",
    "    return input_data\n",
    "\n",
    "\n",
    "def split_data(input_data):\n",
    "    # Assuming your original data is in the 'data' dictionary\n",
    "    split_data = {}\n",
    "\n",
    "    for condition, datasets in input_data.items():\n",
    "        x_train, y_train, x_val, y_val, x_test, y_test = train_val_test_split(datasets[\"x\"], datasets[\"y\"])\n",
    "        split_data[condition] = {\n",
    "            \"train\": {\"x\": x_train, \"y\": y_train},\n",
    "            \"val\": {\"x\": x_val, \"y\": y_val},\n",
    "            \"test\": {\"x\": x_test, \"y\": y_test}\n",
    "        }\n",
    "    return split_data\n",
    "\n",
    "def train_val_test_split(x, y):\n",
    "    total_size = len(x)\n",
    "    train_size = int(0.6 * total_size)  # 60% of the dataset\n",
    "    val_size = int(0.2 * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "    \n",
    "    indices = torch.randperm(total_size)\n",
    "    \n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:train_size + val_size]\n",
    "    test_indices = indices[train_size + val_size:]\n",
    "\n",
    "    x_train = x[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "\n",
    "    x_val = x[val_indices]\n",
    "    y_val = y[val_indices]\n",
    "\n",
    "    x_test = x[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "def save_condition_data(data, save_path):\n",
    "    \"\"\"\n",
    "    Saves train, val, and test sets for each condition in separate .pth files.\n",
    "\n",
    "    Parameters:\n",
    "    data (dict): Nested dictionaries containing the data for each condition.\n",
    "    save_path (str): Path to save the .pth files.\n",
    "    \"\"\"\n",
    "    for index, (condition, datasets) in enumerate(data.items()):\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            if split in datasets:\n",
    "                modified_data = {\n",
    "                    'samples': datasets[split]['x'],\n",
    "                    'labels': datasets[split]['y']\n",
    "                }\n",
    "                file_name = f\"{save_path}/{split}_{index}.pt\"\n",
    "                torch.save(modified_data, file_name)\n",
    "                print(f\"Saved {file_name}\")\n",
    "\n",
    "def process_data(fault_data, save_path= None):\n",
    "    loaded_data =load_data(fault_data)\n",
    "    sampled_data = sample_data(loaded_data)\n",
    "    splitted_data = split_data(sampled_data)\n",
    "    if save_path is not None:\n",
    "        save_condition_data(splitted_data, save_path)\n",
    "    return splitted_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2c7449",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e192e40-147e-4c12-9821-5b9f30880979",
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_dataset = [\"K001\", \"KA01\", \"KA03\", \"KA05\", \"KA07\", \"KI01\", \"KI03\", \"KI07\"]\n",
    "real_dataset = [\"K001\", \"KA04\", \"KB23\", \"KB27\", \"KI04\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40469523-6bcd-4b6b-b34b-34abbae2ed5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PU_Real_Val/train_0.pt\n",
      "Saved PU_Real_Val/val_0.pt\n",
      "Saved PU_Real_Val/test_0.pt\n",
      "Saved PU_Real_Val/train_1.pt\n",
      "Saved PU_Real_Val/val_1.pt\n",
      "Saved PU_Real_Val/test_1.pt\n",
      "Saved PU_Real_Val/train_2.pt\n",
      "Saved PU_Real_Val/val_2.pt\n",
      "Saved PU_Real_Val/test_2.pt\n",
      "Saved PU_Real_Val/train_3.pt\n",
      "Saved PU_Real_Val/val_3.pt\n",
      "Saved PU_Real_Val/test_3.pt\n"
     ]
    }
   ],
   "source": [
    "proc_real_data = process_data(real_dataset, \"PU_Real_Val\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe080423-5741-467c-a1d6-f2ba08d4ce71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PU_Real_Val/train_0.pt\n",
      "Saved PU_Real_Val/val_0.pt\n",
      "Saved PU_Real_Val/test_0.pt\n",
      "Saved PU_Real_Val/train_1.pt\n",
      "Saved PU_Real_Val/val_1.pt\n",
      "Saved PU_Real_Val/test_1.pt\n",
      "Saved PU_Real_Val/train_2.pt\n",
      "Saved PU_Real_Val/val_2.pt\n",
      "Saved PU_Real_Val/test_2.pt\n",
      "Saved PU_Real_Val/train_3.pt\n",
      "Saved PU_Real_Val/val_3.pt\n",
      "Saved PU_Real_Val/test_3.pt\n"
     ]
    }
   ],
   "source": [
    "proc_artificial_dataset_data = process_data(artificial_dataset, \"PU_Art_Val\")\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e02556a-0be3-4c58-8343-53dde83c880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VibrationDataProcessor:\n",
    "    def __init__(self, dataset_list, data_dir, n_points, sample_len, stride):\n",
    "        self.dataset_list = dataset_list\n",
    "        self.data_dir = data_dir\n",
    "        self.N_POINTS = n_points\n",
    "        self.SAMPLE_LEN = sample_len\n",
    "        self.STRIDE = stride\n",
    "        self.data = self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        conditions = {cond: {\"x\": np.zeros((len(self.dataset_list) * 20, self.N_POINTS)), \"y\": np.zeros(len(self.dataset_list) * 20)} for cond in [\"P1\", \"P2\", \"P3\", \"P4\"]}\n",
    "        condition_files = {\"N15_M07_F10\": \"P1\", \"N09_M07_F10\": \"P2\", \"N15_M01_F10\": \"P3\", \"N15_M07_F04\": \"P4\"}\n",
    "\n",
    "        for label, foldername in enumerate(self.dataset_list):\n",
    "            folder_path = os.path.join(self.data_dir, foldername)\n",
    "            file_path_list = glob(os.path.join(folder_path, \"*.mat\"))\n",
    "\n",
    "            files_per_condition = len(file_path_list) // len(condition_files)\n",
    "            for file_index, file_path in enumerate(file_path_list):\n",
    "                for file_cond, cond in condition_files.items():\n",
    "                    if file_cond in file_path:\n",
    "                        vibration_data = self.load_mat(file_path)\n",
    "                        sample_id = (label * 20) + (file_index % files_per_condition)\n",
    "                        conditions[cond][\"x\"][sample_id] = vibration_data\n",
    "                        conditions[cond][\"y\"][sample_id] = label\n",
    "                        break\n",
    "\n",
    "        for cond in conditions.values():\n",
    "            cond[\"x\"], cond[\"y\"] = torch.from_numpy(cond[\"x\"]), torch.from_numpy(cond[\"y\"])\n",
    "\n",
    "        return conditions\n",
    "\n",
    "    def load_mat(self, mat_file):\n",
    "        file_name = os.path.splitext(os.path.basename(mat_file))[0]\n",
    "        mat_file_array = loadmat(mat_file)\n",
    "        vibration_data = mat_file_array[file_name][\"Y\"][0][0][0][6][2][:, :self.N_POINTS]\n",
    "        return np.array(vibration_data)\n",
    "\n",
    "    def sample(self, x, y):\n",
    "        output_x = x.unfold(1, self.SAMPLE_LEN, self.STRIDE)\n",
    "        output_x = output_x.contiguous().view(-1, self.SAMPLE_LEN)\n",
    "        windows_per_sample = output_x.size(0) // y.size(0)\n",
    "        output_y = y.repeat_interleave(windows_per_sample)\n",
    "        return output_x, output_y\n",
    "\n",
    "    def sample_data(self):\n",
    "        for condition, datasets in self.data.items():\n",
    "            sampled_x, sampled_y = self.sample(datasets[\"x\"], datasets[\"y\"])\n",
    "            self.data[condition] = {\"x\": sampled_x, \"y\": sampled_y}\n",
    "\n",
    "    def train_val_test_split(self, x, y):\n",
    "        total_size = len(x)\n",
    "        train_size = int(0.6 * total_size)\n",
    "        val_size = int(0.2 * total_size)\n",
    "\n",
    "        indices = torch.randperm(total_size)\n",
    "\n",
    "        train_indices = indices[:train_size]\n",
    "        val_indices = indices[train_size:train_size + val_size]\n",
    "        test_indices = indices[train_size + val_size:]\n",
    "\n",
    "        x_train = x[train_indices]\n",
    "        y_train = y[train_indices]\n",
    "\n",
    "        x_val = x[val_indices]\n",
    "        y_val = y[val_indices]\n",
    "\n",
    "        x_test = x[test_indices]\n",
    "        y_test = y[test_indices]\n",
    "\n",
    "        return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "    def split_data(self):\n",
    "        split_data = {}\n",
    "        for condition, datasets in self.data.items():\n",
    "            x_train, y_train, x_val, y_val, x_test, y_test = self.train_val_test_split(datasets[\"x\"], datasets[\"y\"])\n",
    "            split_data[condition] = {\"train\": {\"x\": x_train, \"y\": y_train}, \"val\": {\"x\": x_val, \"y\": y_val}, \"test\": {\"x\": x_test, \"y\": y_test}}\n",
    "        return split_data\n",
    "    \n",
    "    def process_data(self):\n",
    "        \"\"\"\n",
    "        Processes the data by loading, sampling, and splitting.\n",
    "        This method orchestrates the entire workflow.\n",
    "        \"\"\"\n",
    "        self.raw_data = self.load_data()  # Load the data\n",
    "        self.sample_data()  # Apply sampling\n",
    "        self.splitted_data = self.split_data()  # Perform train-val-test split\n",
    "        \n",
    "        return self.splitted_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "377881b0-e572-46b6-b08b-52b3cbbbe940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), \"./Paderborn Dataset\")\n",
    "N_POINTS = 249600 #? 64k Sampling rate for 3.9 seconds\n",
    "artificial_dataset = [\"K001\",  \"KA01\", \"KA03\", \"KA05\", \"KA07\", \"KI01\", \"KI03\", \"KI07\"]\n",
    "real_dataset = [\"K001\",     \"KA04\", \"KB23\", \"KB27\", \"KI04\"]\n",
    "SAMPLE_LEN = 1024\n",
    "STRIDE = SAMPLE_LEN \n",
    "processor = VibrationDataProcessor(artificial_dataset, data_dir, N_POINTS, SAMPLE_LEN, STRIDE)\n",
    "processed_art_data = processor.process_data()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe590034-c6b3-444d-a06a-83c9f9cd4ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0885,  0.1038,  0.0885,  ...,  0.0488, -0.0427, -0.0305],\n",
       "        [ 2.5665, -0.1892,  0.9277,  ..., -0.1038, -0.0977, -0.0854],\n",
       "        [-0.1129, -0.1770, -0.2106,  ...,  0.2106,  0.4578,  0.5737],\n",
       "        ...,\n",
       "        [-0.0427,  0.0793,  0.1190,  ...,  0.0336, -0.0153, -0.0610],\n",
       "        [ 0.0397, -0.0427, -0.1190,  ..., -0.1373, -0.0366,  0.1221],\n",
       "        [-0.1068, -0.0671,  0.2686,  ..., -0.6836, -0.4608, -0.5859]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_art_data[\"P1\"][\"train\"][\"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81bffbe0-652e-4add-be76-291ce2dfefd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1343,  0.0427,  0.2197,  ..., -0.1587,  0.1129,  0.1984],\n",
       "        [-0.0153,  0.0092, -0.0183,  ...,  0.1465,  0.1556,  0.0397],\n",
       "        [ 0.0336, -0.5157,  0.0641,  ..., -0.0793, -0.0671,  0.0519],\n",
       "        ...,\n",
       "        [-0.0549, -0.0244,  0.0427,  ...,  0.0854,  0.0092,  0.2289],\n",
       "        [-0.1801,  0.0610, -0.0275,  ..., -0.2747, -0.2106, -0.1648],\n",
       "        [-0.2319, -0.4089, -0.1709,  ..., -0.5554, -0.5432, -0.4303]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_art_data[\"P1\"][\"train\"][\"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f91ea8ac-0c70-4013-868c-930ce6e0e048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "are_close = torch.allclose(processed_art_data[\"P1\"][\"train\"][\"x\"], proc_art_data[\"P1\"][\"train\"][\"x\"], atol=1e-04)  # atol is the tolerance level\n",
    "print(are_close)  # This will print True if they are approximately equal within the tolerance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77b507e6-a82f-4ce6-9d71-edf7a0358151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['P1', 'P2', 'P3', 'P4']),\n",
       " dict_keys(['train', 'val', 'test']),\n",
       " dict_keys(['x', 'y']))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_art_data.keys(), proc_art_data[\"P1\"].keys(), proc_art_data[\"P1\"][\"train\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb6df18",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "266476ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "SAMPLE_LEN = 1024\n",
    "STRIDE = SAMPLE_LEN #! NO overlap\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.2\n",
    "# N_FOLDS = 5 #! Validation set not created\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8845c783",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f284f979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real domain\n",
    "Real_Normal_train = {\"samples\": Real_Normal_x_train, \"labels\": Real_Normal_y_train}\n",
    "Real_Normal_test = {\"samples\": Real_Normal_x_test, \"labels\": Real_Normal_y_test}\n",
    "\n",
    "Real_Rotate_train = {\"samples\": Real_Rotate_x_train, \"labels\": Real_Rotate_y_train}\n",
    "Real_Rotate_test = {\"samples\": Real_Rotate_x_test, \"labels\": Real_Rotate_y_test}\n",
    "\n",
    "Real_Load_train = {\"samples\": Real_Load_x_train, \"labels\": Real_Load_y_train}\n",
    "Real_Load_test = {\"samples\": Real_Load_x_test, \"labels\": Real_Load_y_test}\n",
    "\n",
    "Real_Radial_train = {\"samples\": Real_Radial_x_train, \"labels\": Real_Radial_y_train}\n",
    "Real_Radial_test = {\"samples\": Real_Radial_x_test, \"labels\": Real_Radial_y_test}\n",
    "\n",
    "# Artificial domain\n",
    "Artificial_Normal_train = {\"samples\": Artificial_Normal_x_train, \"labels\": Artificial_Normal_y_train}\n",
    "Artificial_Normal_test = {\"samples\": Artificial_Normal_x_test, \"labels\": Artificial_Normal_y_test}\n",
    "\n",
    "Artificial_Rotate_train = {\"samples\": Artificial_Rotate_x_train, \"labels\": Artificial_Rotate_y_train}\n",
    "Artificial_Rotate_test = {\"samples\": Artificial_Rotate_x_test, \"labels\": Artificial_Rotate_y_test}\n",
    "\n",
    "Artificial_Load_train = {\"samples\": Artificial_Load_x_train, \"labels\": Artificial_Load_y_train}\n",
    "Artificial_Load_test = {\"samples\": Artificial_Load_x_test, \"labels\": Artificial_Load_y_test}\n",
    "\n",
    "Artificial_Radial_train = {\"samples\": Artificial_Radial_x_train, \"labels\": Artificial_Radial_y_train}\n",
    "Artificial_Radial_test = {\"samples\": Artificial_Radial_x_test, \"labels\": Artificial_Radial_y_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4fd773",
   "metadata": {},
   "source": [
    "# Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f77c7cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parent directory\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "Real_dir = os.path.join(parent_dir, \"dataset\", \"PU_Real\")\n",
    "Artificial_dir = os.path.join(parent_dir, \"dataset\", \"PU_Artificial\")\n",
    "\n",
    "if not os.path.exists(Real_dir):\n",
    "    os.makedirs(Real_dir)\n",
    "\n",
    "if not os.path.exists(Artificial_dir):\n",
    "    os.makedirs(Artificial_dir)\n",
    "\n",
    "torch.save(Real_Normal_train, os.path.join(Real_dir, \"train_Normal.pt\"))\n",
    "torch.save(Real_Normal_test, os.path.join(Real_dir, \"test_Normal.pt\"))\n",
    "torch.save(Real_Rotate_train, os.path.join(Real_dir, \"train_Rotate.pt\"))\n",
    "torch.save(Real_Rotate_test, os.path.join(Real_dir, \"test_Rotate.pt\"))\n",
    "torch.save(Real_Load_train, os.path.join(Real_dir, \"train_Load.pt\"))\n",
    "torch.save(Real_Load_test, os.path.join(Real_dir, \"test_Load.pt\"))\n",
    "torch.save(Real_Radial_train, os.path.join(Real_dir, \"train_Radial.pt\"))\n",
    "torch.save(Real_Radial_test, os.path.join(Real_dir, \"test_Radial.pt\"))\n",
    "\n",
    "torch.save(Artificial_Normal_train, os.path.join(Artificial_dir, \"train_Normal.pt\"))\n",
    "torch.save(Artificial_Normal_test, os.path.join(Artificial_dir, \"test_Normal.pt\"))\n",
    "torch.save(Artificial_Rotate_train, os.path.join(Artificial_dir, \"train_Rotate.pt\"))\n",
    "torch.save(Artificial_Rotate_test, os.path.join(Artificial_dir, \"test_Rotate.pt\"))\n",
    "torch.save(Artificial_Load_train, os.path.join(Artificial_dir, \"train_Load.pt\"))\n",
    "torch.save(Artificial_Load_test, os.path.join(Artificial_dir, \"test_Load.pt\"))\n",
    "torch.save(Artificial_Radial_train, os.path.join(Artificial_dir, \"train_Radial.pt\"))\n",
    "torch.save(Artificial_Radial_test, os.path.join(Artificial_dir, \"test_Radial.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
