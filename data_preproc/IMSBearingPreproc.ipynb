{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90e83fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import py7zr\n",
    "import random\n",
    "import rarfile\n",
    "from scipy.io import loadmat\n",
    "import torch\n",
    "import urllib.request\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f316a4",
   "metadata": {},
   "source": [
    "# Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8058815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e9e01",
   "metadata": {},
   "source": [
    "# Download and extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3d55798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://phm-datasets.s3.amazonaws.com/NASA/4.+Bearings.zip\n",
      "Extracting IMS.zip\n",
      "Extract C:\\Work\\ASTAR\\codes\\CDA\\CDA\\data_preproc\\IMS\\IMS\\4. Bearings\\IMS\\1st_test.rar manually\n",
      "Extract C:\\Work\\ASTAR\\codes\\CDA\\CDA\\data_preproc\\IMS\\IMS\\4. Bearings\\IMS\\2nd_test.rar manually\n",
      "Extract C:\\Work\\ASTAR\\codes\\CDA\\CDA\\data_preproc\\IMS\\IMS\\4. Bearings\\IMS\\3rd_test.rar manually\n"
     ]
    }
   ],
   "source": [
    "# IMS bearings dataset\n",
    "IMS_links = {\n",
    "    'IMS': 'https://phm-datasets.s3.amazonaws.com/NASA/4.+Bearings.zip'\n",
    "}\n",
    "\n",
    "def download_and_extract(file_name, url, folder_path, dtype, extract_function):\n",
    "    while True:\n",
    "        try:\n",
    "            print(f\"Downloading {url}\")\n",
    "            urllib.request.urlretrieve(url, os.path.join(folder_path, f'{file_name}{dtype}'))\n",
    "            print(f'Extracting {file_name}{dtype}')\n",
    "            extract_function(folder_path, file_name)\n",
    "            break  # Exit the loop if the download is successful\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {url}: {e}\")\n",
    "\n",
    "def extract_rar(folder, file_name):\n",
    "    print(f'Extract {os.path.join(folder, file_name)}.rar manually')\n",
    "#     with rarfile.RarFile(os.path.join(folder, f'{file_name}.rar')) as rf:\n",
    "#         rf.extractall(os.path.join(folder, file_name))\n",
    "    pass\n",
    "        \n",
    "def extract_zip(folder, file_name):\n",
    "    with zipfile.ZipFile(os.path.join(folder, f'{file_name}.zip'), 'r') as zip_ref:\n",
    "        zip_ref.extractall(os.path.join(folder, file_name))\n",
    "        \n",
    "def extract_7zip(folder, file_name):\n",
    "    with py7zr.SevenZipFile(os.path.join(folder, f'{file_name}.7z'), mode='r') as sevenzip:\n",
    "        sevenzip.extractall(os.path.join(folder, file_name))\n",
    "        \n",
    "# Download & Extract IMS dataset\n",
    "folder_path = os.path.join(os.getcwd(), 'IMS')\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "for file_name, url_link in IMS_links.items():\n",
    "    if not os.path.exists(os.path.join(folder_path, file_name)):\n",
    "        download_and_extract(file_name, url_link, folder_path, '.zip', extract_zip)\n",
    "\n",
    "# Unzip the 7zip folder inside zipfolder & extract .rar files inside inner 7zip folder\n",
    "if not os.path.exists(os.path.join(folder_path, 'IMS', '4. Bearings', 'IMS')):\n",
    "    extract_7zip(os.path.join(folder_path, 'IMS', '4. Bearings'), 'IMS')\n",
    "    extract_rar(os.path.join(folder_path, 'IMS', '4. Bearings', 'IMS'), '1st_test')\n",
    "    extract_rar(os.path.join(folder_path, 'IMS', '4. Bearings', 'IMS'), '2nd_test')\n",
    "    extract_rar(os.path.join(folder_path, 'IMS', '4. Bearings', 'IMS'), '3rd_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be31f69b",
   "metadata": {},
   "source": [
    "# Process IMS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7b21fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already converted filenames\n",
      "Already converted filenames\n"
     ]
    }
   ],
   "source": [
    "# Convert file from YYYY.MM.DD.hh.mm.ss format to YYYYMMDDhhmmss.txt files to make it easier to sort\n",
    "def convert_filename_to_integer(filename):\n",
    "    parts = filename.split('.')\n",
    "    return int(\"\".join(parts))\n",
    "\n",
    "def rename_files_in_directory(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        try:\n",
    "            new_filename = str(convert_filename_to_integer(filename))\n",
    "        except ValueError:\n",
    "            print(f\"Already converted filenames\")\n",
    "            break\n",
    "        os.rename(os.path.join(directory, filename), os.path.join(directory, f'{new_filename}.txt'))\n",
    " \n",
    "test_1_path = os.path.join(os.getcwd(), 'IMS/IMS/4. Bearings/IMS/1st_test')\n",
    "test_2_path = os.path.join(os.getcwd(), 'IMS/IMS/4. Bearings/IMS/2nd_test')\n",
    "rename_files_in_directory(test_1_path)\n",
    "rename_files_in_directory(test_2_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acc9963",
   "metadata": {},
   "source": [
    "Following the works of https://hal.science/hal-01715193/document, the following labels are used\n",
    "\n",
    "        Inner:   2003/11/22/00:06:56 - 2003/11/25/23:39:56 (30 days into the run of test 1, channel 5)\n",
    "        Roller:  2003/11/14/11:02:17 - 2003/11/25/23:39:56 (23 days into the run of test 1, channel 7)\n",
    "        Outer:   2004/02/14/22:22:39 - 2004/02/19/06:22:39 (2.5 days into the run of test 2, channel 1)\n",
    "        \n",
    "For the healthy states, the first half of the snapshots before it was considered faulty will be used. Additionally, the first 10% of the first snapshot will be treated as early operation and excluded similarly to this work: https://papers.phmsociety.org/index.php/phme/article/download/2947/1761\n",
    "\n",
    "        Healthy: 2003/10/29/17:29:46 - 2003/11/10/12:05:58 (test 1, channel 5)\n",
    "        Roller:  2003/10/22/23:04:13 - 2003/11/01/19:21:44 (test 1, channel 7)\n",
    "        Outer:   2004/02/12/16:22:39 - 2004/02/13/16:32:39 (test 2, channel 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79b9ba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_1 = os.listdir(test_1_path)\n",
    "file_list_2 = os.listdir(test_2_path)\n",
    "\n",
    "Inner_files = file_list_1[1735:]\n",
    "Roller_files = file_list_1[894:]\n",
    "Outer_files = file_list_2[363:]\n",
    "\n",
    "Healthy_bearing_3 = file_list_1[173:867]\n",
    "Healthy_bearing_4 = file_list_1[88:485]\n",
    "Healthy_bearing_1 = file_list_2[35:181]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fe92edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70820, 1, 1024]) torch.Size([70820])\n"
     ]
    }
   ],
   "source": [
    "def sliding_window_subsample(tensor, window_size=1024, step=1024):\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    return tensor.unfold(2, window_size, step).transpose(0, 1).transpose(1, 2).squeeze(0)\n",
    "\n",
    "def read_files(folder, file_list, channels, label):\n",
    "    x, y = [], []\n",
    "    for file_name in file_list:\n",
    "        data = pd.read_csv(os.path.join(folder, file_name), sep='\\t', header=None)\n",
    "        tensor_data = torch.tensor(data.iloc[:, channels].values, dtype=torch.float).unsqueeze(0)\n",
    "        \n",
    "        subsampled_data = sliding_window_subsample(tensor_data, window_size=1024, step=1024)\n",
    "        labels = torch.full((subsampled_data.shape[0],), label)\n",
    "        \n",
    "        x.append(subsampled_data)\n",
    "        y.append(labels)\n",
    "        \n",
    "    x_tensor = torch.cat(x, dim=0)\n",
    "    y_tensor = torch.cat(y, dim=0)\n",
    "    \n",
    "    return x_tensor, y_tensor\n",
    "\n",
    "# Dataset 1 has 2 channels for each bearing but both channels were noted to be similar enough\n",
    "Inner_x, Inner_y = read_files(test_1_path, Inner_files, channels=5, label=1)\n",
    "Roller_x, Roller_y = read_files(test_1_path, Roller_files, channels=7, label=1)\n",
    "Outer_x, Outer_y = read_files(test_2_path, Outer_files, channels=0, label=1)\n",
    "\n",
    "Healthy_bearing_3_x, Healthy_bearing_3_y = read_files(test_1_path, Healthy_bearing_3, channels=5, label=0)\n",
    "Healthy_bearing_4_x, Healthy_bearing_4_y = read_files(test_1_path, Healthy_bearing_4, channels=7, label=0)\n",
    "Healthy_bearing_1_x, Healthy_bearing_1_y = read_files(test_2_path, Healthy_bearing_1, channels=0, label=0)\n",
    "\n",
    "# Combine all class into a tensor\n",
    "combined_x = torch.cat((Inner_x, Roller_x, Outer_x, Healthy_bearing_3_x, Healthy_bearing_4_x, Healthy_bearing_1_x), dim=0)\n",
    "combined_y = torch.cat((Inner_y, Roller_y, Outer_y, Healthy_bearing_3_y, Healthy_bearing_4_y, Healthy_bearing_1_y), dim=0)\n",
    "\n",
    "print(combined_x.shape, combined_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3b3db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the tensor into training, validation and testing\n",
    "dataset = torch.utils.data.TensorDataset(combined_x, combined_y) # Combine x and y to ensure both are split in the same way\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ab5607c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42492, 1, 1024]) torch.Size([42492])\n",
      "torch.Size([14164, 1, 1024]) torch.Size([14164])\n",
      "torch.Size([14164, 1, 1024]) torch.Size([14164])\n"
     ]
    }
   ],
   "source": [
    "# Split the tensor into training, validation and testing\n",
    "dataset = torch.utils.data.TensorDataset(combined_x, combined_y) # Combine x and y to ensure both are split in the same way\n",
    "\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.6 * total_size)\n",
    "val_size = int(0.2 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Split x and y\n",
    "def split_xy(dataset):\n",
    "    x, y = [], []\n",
    "    for x_tensor, y_tensor in dataset:\n",
    "        x.append(x_tensor)\n",
    "        y.append(y_tensor)\n",
    "    # Convert lists to tensors\n",
    "    x = torch.stack(x)\n",
    "    y = torch.stack(y)\n",
    "    return x, y\n",
    "\n",
    "train_x, train_y = split_xy(train_dataset)\n",
    "val_x, val_y = split_xy(val_dataset)\n",
    "test_x, test_y = split_xy(test_dataset)\n",
    "\n",
    "# Check the shapes of the splits\n",
    "print(train_x.shape, train_y.shape)  # Training set\n",
    "print(val_x.shape, val_y.shape)      # Validation set\n",
    "print(test_x.shape, test_y.shape)    # Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e7abbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the datasets\n",
    "training = {\"samples\": train_x,  \"labels\":train_y}\n",
    "validation = {\"samples\": val_x,  \"labels\":val_y}\n",
    "testing = {\"samples\": test_x,  \"labels\":test_y}\n",
    "\n",
    "torch.save(training, os.path.join(os.getcwd(), 'IMS', 'train.pt'))\n",
    "torch.save(validation, os.path.join(os.getcwd(), 'IMS', 'val.pt'))\n",
    "torch.save(testing, os.path.join(os.getcwd(), 'IMS', 'test.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
